{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   serial_no    image_name                                           text_ocr  \\\n",
      "0          0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
      "1          1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
      "2          2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
      "3          3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
      "4          4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
      "\n",
      "                                      text_corrected overall_sentiment  \n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...     very_positive  \n",
      "1  The best of #10 YearChallenge! Completed in le...     very_positive  \n",
      "2  Sam Thorne @Strippin ( Follow Follow Saw every...          positive  \n",
      "3              10 Year Challenge - Sweet Dee Edition          positive  \n",
      "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...           neutral  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load CSV file\n",
    "csv_file_path = './labels.csv'\n",
    "images_folder_path = './images'\n",
    "\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Function to load images based on serial number\n",
    "def load_image(image_name):\n",
    "    image_path = os.path.join(images_folder_path, image_name)\n",
    "    if os.path.exists(image_path):\n",
    "        return Image.open(image_path)\n",
    "    raise FileNotFoundError(f\"No image found for {image_name}\")\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to a standard size\n",
    "    transforms.ToTensor(),          # Convert images to tensor\n",
    "])\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image):\n",
    "    return transform(image)\n",
    "\n",
    "# Example: Load and preprocess the first image\n",
    "image_name = data.iloc[0]['image_name']\n",
    "image = load_image(image_name)\n",
    "preprocessed_image = preprocess_image(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ced826910034cebaa068b01e05ccbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\OneDrive\\Desktop\\New folder\\meme generator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rishi\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe702c0fb2464840a1726673f76f23e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bff669f24ef4c1abb66364958bcb47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\OneDrive\\Desktop\\New folder\\meme generator\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1582d16070e3475fa18752c0d465084b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "    return tokens.input_ids, tokens.attention_mask\n",
    "\n",
    "# Example: Preprocess text\n",
    "text_ocr = data.iloc[0]['text_ocr']\n",
    "input_ids, attention_mask = preprocess_text(text_ocr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(128, 256), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256 * 256 * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 3, 256, 256)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(256 * 256 * 3, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x.view(-1, 256 * 256 * 3))\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   serial_no    image_name                                           text_ocr  \\\n",
      "0          0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n",
      "1          1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n",
      "2          2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n",
      "3          3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n",
      "4          4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n",
      "\n",
      "                                      text_corrected overall_sentiment  \n",
      "0  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...     very_positive  \n",
      "1  The best of #10 YearChallenge! Completed in le...     very_positive  \n",
      "2  Sam Thorne @Strippin ( Follow Follow Saw every...          positive  \n",
      "3              10 Year Challenge - Sweet Dee Edition          positive  \n",
      "4  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...           neutral  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishi\\OneDrive\\Desktop\\New folder\\meme generator\\venv\\Lib\\site-packages\\PIL\\Image.py:1000: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10000] - Loss D: 3.7906062442061277e-36, Loss G: 98.67425537109375\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Load CSV file\n",
    "csv_file_path = './labels.csv'\n",
    "images_folder_path = './images'\n",
    "\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n",
    "\n",
    "# Function to load images based on image name\n",
    "def load_image(image_name):\n",
    "    image_path = os.path.join(images_folder_path, image_name)\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')  # Convert image to RGB\n",
    "            return img\n",
    "        except (OSError, IOError) as e:\n",
    "            print(f\"Error loading image {image_name}: {e}\")\n",
    "            return None\n",
    "    raise FileNotFoundError(f\"No image found for {image_name}\")\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to a standard size\n",
    "    transforms.ToTensor(),          # Convert images to tensor\n",
    "])\n",
    "\n",
    "# Preprocess images\n",
    "def preprocess_image(image):\n",
    "    return transform(image)\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    tokens = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=128)\n",
    "    return tokens.input_ids.float(), tokens.attention_mask.float()  # Convert to float\n",
    "\n",
    "# Define the generator and discriminator for the Conditional GAN\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(128, 256), \n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256 * 256 * 3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 3, 256, 256)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(256 * 256 * 3, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x.view(-1, 256 * 256 * 3))\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Define loss and optimizers\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        # Get batch data\n",
    "        batch_data = data.iloc[i:i+batch_size]\n",
    "        images = []\n",
    "        texts = []\n",
    "\n",
    "        for _, row in batch_data.iterrows():\n",
    "            img = load_image(row['image_name'])\n",
    "            if img is not None:\n",
    "                images.append(preprocess_image(img))\n",
    "                text_ocr = row['text_ocr']\n",
    "                input_ids, _ = preprocess_text(text_ocr)\n",
    "                texts.append(input_ids)\n",
    "\n",
    "        if len(images) == 0:\n",
    "            continue\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        texts = torch.cat(texts)\n",
    "        \n",
    "        # Labels for real and fake images\n",
    "        real_labels = torch.ones(images.size(0), 1)  # Adjust to match the actual batch size\n",
    "        fake_labels = torch.zeros(images.size(0), 1)  # Adjust to match the actual batch size\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        outputs = discriminator(images)\n",
    "        loss_d_real = criterion(outputs, real_labels)\n",
    "        loss_d_real.backward()\n",
    "\n",
    "        noise = torch.randn(images.size(0), 128)  # Adjust to match the actual batch size\n",
    "        fake_images = generator(texts)\n",
    "        outputs = discriminator(fake_images.detach())\n",
    "        loss_d_fake = criterion(outputs, fake_labels)\n",
    "        loss_d_fake.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        optimizer_g.zero_grad()\n",
    "        outputs = discriminator(fake_images)\n",
    "        loss_g = criterion(outputs, real_labels)\n",
    "        loss_g.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}] - Loss D: {loss_d_real + loss_d_fake}, Loss G: {loss_g}')\n",
    "        # Save models or generate sample images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images from text\n",
    "def generate_image(text):\n",
    "    input_ids, _ = preprocess_text(text)\n",
    "    with torch.no_grad():\n",
    "        generated_image = generator(input_ids)\n",
    "    return transforms.ToPILImage()(generated_image.squeeze())\n",
    "\n",
    "# Example: Generate an image\n",
    "new_text = \"A beautiful landscape with mountains\"\n",
    "generated_image = generate_image(new_text)\n",
    "generated_image.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
